{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04db9b5f",
   "metadata": {},
   "source": [
    "##Training Loop (Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906d8dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading PSPNet...\n",
      "üìÇ Found 10192 images. Processing 10192 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2046063b245438faecd306bef852e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Saved: x_train_normal_bestpy.npy | Shape: (10182, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PART 1: PREPARE DATA FROM NORMAL FOLDER\n",
    "# ==========================================\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ‚ö†Ô∏è ‡πÅ‡∏Å‡πâ Path ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Normal ‡∏Ç‡∏≠‡∏á COVID-19 Database\n",
    "RAW_DATA_PATH = r\"COVID-19_Radiography_Dataset\\Normal\\images\" \n",
    "\n",
    "SAVE_NPY_PATH = \"x_train_normal_bestpy.npy\"\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î Mask Model (PSPNet) ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô bestpy\n",
    "print(\"‚è≥ Loading PSPNet...\")\n",
    "seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
    "seg_model.eval()\n",
    "\n",
    "def get_bestpy_mask(img):\n",
    "    # ‡∏™‡∏π‡∏ï‡∏£ Mask ‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å bestpy (L+R ‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏´‡∏±‡∏ß‡πÉ‡∏à)\n",
    "    img = xrv.datasets.normalize(img, 255)\n",
    "    if len(img.shape) == 3: img = img.mean(2) # ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô RGB ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Gray\n",
    "    img = img[None, ...] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        out = seg_model(img_tensor)\n",
    "        pred = out[0].numpy()\n",
    "    \n",
    "    # ‡∏£‡∏ß‡∏°‡∏õ‡∏≠‡∏î‡∏ã‡πâ‡∏≤‡∏¢‡∏Ç‡∏ß‡∏≤\n",
    "    mask = (pred[4] + pred[5]) > 0.5\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "    return cv2.resize(mask, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def preprocess(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: return None\n",
    "    \n",
    "    # Resize ‡∏Ç‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å\n",
    "    img_512 = cv2.resize(img, (512, 512))\n",
    "    \n",
    "    # 1. Masking (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô bestpy)\n",
    "    mask = get_bestpy_mask(img_512)\n",
    "    \n",
    "    # 2. Enhancement (CLAHE) - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô bestpy\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img_enh = clahe.apply(img_512)\n",
    "    \n",
    "    # ‡∏ï‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏¥‡πâ‡∏á\n",
    "    img_masked = cv2.bitwise_and(img_enh, img_enh, mask=mask)\n",
    "    \n",
    "    # 3. Final Resize & Normalize\n",
    "    img_final = cv2.resize(img_masked, (IMG_SIZE, IMG_SIZE))\n",
    "    img_final = img_final.astype('float32') / 255.0\n",
    "    \n",
    "    return img_final\n",
    "\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£\n",
    "files = glob.glob(os.path.join(RAW_DATA_PATH, \"*.png\")) + glob.glob(os.path.join(RAW_DATA_PATH, \"*.jpg\"))\n",
    "print(f\"üìÇ Found {len(files)} images. Processing {len(files)} images...\")\n",
    "\n",
    "data = []\n",
    "for f in tqdm(files[:]):\n",
    "    try:\n",
    "        p = preprocess(f)\n",
    "        if p is not None and np.sum(p) > 0: # ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏†‡∏≤‡∏û‡∏î‡∏≥‡∏•‡πâ‡∏ß‡∏ô\n",
    "            data.append(p)\n",
    "    except: continue\n",
    "\n",
    "# Save .npy\n",
    "data = np.array(data)\n",
    "data = np.expand_dims(data, axis=-1) # (N, 128, 128, 1)\n",
    "np.save(SAVE_NPY_PATH, data)\n",
    "print(f\"‚úÖ Data Saved: {SAVE_NPY_PATH} | Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bfdf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU Detected: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler # ‚ö° ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True # ‚ö° ‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è GPU not found! Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db4bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cu130\n",
      "CUDA Available: True\n",
      "CUDA Version (Torch built with): 13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version (Torch built with): {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c965e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU Detected: NVIDIA GeForce RTX 4060\n",
      "‚è≥ Loading Data...\n",
      "üöÄ Start GPU Turbo Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff965d49be5d459d8d7694c42ef47ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Avg Loss: 0.46997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1532c7a6046e45bf96ed2d62d1f84f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Avg Loss: 0.03531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e09e29e3c034f43a228bc5f471249bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Avg Loss: 0.01796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8366a25d08624cebb89a825e04ca9890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Avg Loss: 0.01251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12b61b5e93c4516b976b9d8a2555703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Avg Loss: 0.01094\n",
      "üíæ Model Saved to: lung_diffusion_model_new\n",
      "‚úÖ Training Complete! Ready for Inference.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PART 2: GPU TURBO TRAINING (Fix Deprecation Warning)\n",
    "# ==========================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# ================= 1. CONFIG =================\n",
    "DATA_PATH = \"x_train_normal_bestpy.npy\" \n",
    "MODEL_SAVE_DIR = \"lung_diffusion_model_new\"\n",
    "\n",
    "BATCH_SIZE = 64 \n",
    "IMG_SIZE = 128\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è GPU not found! Training will be slow.\")\n",
    "\n",
    "# ================= 2. LOAD DATA =================\n",
    "print(\"‚è≥ Loading Data...\")\n",
    "data = np.load(DATA_PATH)\n",
    "data = np.transpose(data, (0, 3, 1, 2)) # (N, 1, 128, 128)\n",
    "dataset = TensorDataset(torch.Tensor(data))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    pin_memory=True if device == \"cuda\" else False,\n",
    "    num_workers=0 # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ IProgress ‡πÉ‡∏ô Windows\n",
    ")\n",
    "\n",
    "# ================= 3. SETUP MODEL =================\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256, 256),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * EPOCHS,\n",
    ")\n",
    "\n",
    "# ‚ö° ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡πÉ‡∏ä‡πâ torch.amp.GradScaler ‡πÅ‡∏ó‡∏ô torch.cuda.amp.GradScaler\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# ================= 4. TRAINING LOOP =================\n",
    "print(\"üöÄ Start GPU Turbo Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    # ‡πÉ‡∏ä‡πâ tqdm ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ IProgress\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        clean_images = batch[0].to(device, non_blocking=True)\n",
    "        \n",
    "        # 1. Add Noise\n",
    "        noise = torch.randn(clean_images.shape).to(device)\n",
    "        bs = clean_images.shape[0]\n",
    "        timesteps = torch.randint(0, 1000, (bs,), device=device).long()\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Mixed Precision Forward Pass (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Syntax ‡πÉ‡∏´‡∏°‡πà)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            noise_pred = model(noisy_images, timesteps).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # 3. Backward Pass with Scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "        \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.5f}\")\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(MODEL_SAVE_DIR)\n",
    "print(f\"üíæ Model Saved to: {MODEL_SAVE_DIR}\")\n",
    "print(\"‚úÖ Training Complete! Ready for Inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5ef3e",
   "metadata": {},
   "source": [
    "##Use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b06def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Models...\n",
      "‚úÖ Models Ready!\n",
      "üöÄ Starting Analysis with YOUR Custom Function...\n",
      "Processing: 2268 (10 scans)\n",
      "Processing: 2272 (18 scans)\n",
      "Processing: 2273 (4 scans)\n",
      "Processing: 2278 (5 scans)\n",
      "Processing: 2279 (6 scans)\n",
      "Processing: 2283 (6 scans)\n",
      "Processing: 2285 (5 scans)\n",
      "Processing: 2286 (9 scans)\n",
      "Processing: 2288 (5 scans)\n",
      "Processing: 2289 (6 scans)\n",
      "Processing: 2290 (13 scans)\n",
      "Processing: 2299 (2 scans)\n",
      "Processing: 2300 (10 scans)\n",
      "Processing: 2303 (3 scans)\n",
      "Processing: 2304 (6 scans)\n",
      "Processing: 2310 (1 scans)\n",
      "Processing: 2312 (5 scans)\n",
      "Processing: 2315 (5 scans)\n",
      "Processing: 2316 (2 scans)\n",
      "Processing: 2317 (7 scans)\n",
      "Processing: 2320 (2 scans)\n",
      "Processing: 2321 (9 scans)\n",
      "Processing: 2328 (8 scans)\n",
      "Processing: 2336 (6 scans)\n",
      "Processing: 2337 (4 scans)\n",
      "Processing: 2338 (4 scans)\n",
      "Processing: 2131 (7 scans)\n",
      "Processing: 2140 (25 scans)\n",
      "Processing: 2157 (18 scans)\n",
      "Processing: 2167 (14 scans)\n",
      "Processing: 2179 (26 scans)\n",
      "Processing: 2199 (14 scans)\n",
      "Processing: 2200 (13 scans)\n",
      "Processing: 2203 (3 scans)\n",
      "Processing: 2214 (9 scans)\n",
      "Processing: 2219 (11 scans)\n",
      "Processing: 2226 (10 scans)\n",
      "Processing: 2230 (17 scans)\n",
      "Processing: 2231 (13 scans)\n",
      "Processing: 2241 (7 scans)\n",
      "Processing: 2266 (4 scans)\n",
      "Processing: 2277 (9 scans)\n",
      "Processing: 2287 (2 scans)\n",
      "Processing: 2291 (8 scans)\n",
      "Processing: 2292 (3 scans)\n",
      "Processing: 2306 (19 scans)\n",
      "Processing: 2319 (10 scans)\n",
      "Processing: 2332 (19 scans)\n",
      "Processing: 2345 (16 scans)\n",
      "Processing: 2359 (12 scans)\n",
      "Processing: 2380 (13 scans)\n",
      "Processing: 2404 (7 scans)\n",
      "\n",
      "‚úÖ Final Report Saved: Final_User_Code_Integration\\Clinical_Data\\Clinical_Data.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.family'] = 'Tahoma'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================= 1. CONFIGURATION =================\n",
    "class Config:\n",
    "    ROOT_DIR = \"Chest xray CP class\"\n",
    "    MODEL_PATH = \"lung_diffusion_model\"\n",
    "    \n",
    "    BASE_OUTPUT = \"Final_User_Code_Integration\"\n",
    "    DIRS = {\n",
    "        \"heatmaps\": os.path.join(BASE_OUTPUT, \"Heatmaps\"),\n",
    "        \"timelines\": os.path.join(BASE_OUTPUT, \"Timelines\"),\n",
    "        \"comparisons\": os.path.join(BASE_OUTPUT, \"Comparisons\"),\n",
    "        \"csv\": os.path.join(BASE_OUTPUT, \"Clinical_Data\")\n",
    "    }\n",
    "    \n",
    "    # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Override ‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô analyze_lesion_score ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ñ‡∏∏‡∏ì\n",
    "    IMG_SIZE = 128\n",
    "    START_TIMESTEP = 180 \n",
    "    THRESHOLD = 0.15\n",
    "    SEED = 42\n",
    "\n",
    "for d in Config.DIRS.values():\n",
    "    if not os.path.exists(d): os.makedirs(d)\n",
    "\n",
    "# ================= 2. MODEL LOADING =================\n",
    "print(\"‚è≥ Loading Models...\")\n",
    "seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
    "seg_model.eval()\n",
    "\n",
    "if os.path.exists(Config.MODEL_PATH):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    diffusion_model = UNet2DModel.from_pretrained(Config.MODEL_PATH).to(device)\n",
    "    diffusion_model.eval()\n",
    "    print(\"‚úÖ Models Ready!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Diffusion\")\n",
    "\n",
    "# ================= 3. HELPER FUNCTIONS (Mask & Enhance) =================\n",
    "# ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Mask L+R)\n",
    "def get_masks(img_numpy_uint8):\n",
    "    try:\n",
    "        h, w = img_numpy_uint8.shape\n",
    "        img_norm = xrv.datasets.normalize(img_numpy_uint8, 255) \n",
    "        img_resized = cv2.resize(img_norm, (512, 512))\n",
    "        img_tensor = torch.from_numpy(img_resized)[None, None, ...].float()\n",
    "        with torch.no_grad(): outputs = seg_model(img_tensor)\n",
    "        pred = outputs[0].numpy()\n",
    "        def resize_m(m): return cv2.resize(m, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        mask_l = (resize_m(pred[4]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_r = (resize_m(pred[5]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_total = cv2.bitwise_or(mask_l, mask_r)\n",
    "        \n",
    "        return mask_total, mask_l, mask_r\n",
    "    except:\n",
    "        z = np.zeros(img_numpy_uint8.shape, dtype=np.uint8)\n",
    "        return z, z, z\n",
    "\n",
    "def enhance_lung_clarity(img_gray, lung_mask):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(img_gray)\n",
    "    return cv2.bitwise_and(enhanced, enhanced, mask=lung_mask)\n",
    "\n",
    "def process_image(img_path):\n",
    "    if not os.path.exists(img_path): return None, None\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: return None, None\n",
    "    \n",
    "    mask_total, _, _ = get_masks(img)\n",
    "    if np.sum(mask_total) == 0: enhanced = img\n",
    "    else: enhanced = enhance_lung_clarity(img, mask_total)\n",
    "        \n",
    "    resized = cv2.resize(enhanced, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "    img_tensor = torch.tensor(resized).float() / 255.0\n",
    "    \n",
    "    # Return Tensor and Original Numpy (for visualization)\n",
    "    return img_tensor.unsqueeze(0).unsqueeze(0).to(device), img\n",
    "\n",
    "# ================= 4. CORE FUNCTION (FROM YOUR CODE) =================\n",
    "def analyze_lesion_score(img_path, model, scheduler, start_timestep=180, threshold=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 4 ‡∏≠‡∏¢‡πà‡∏≤‡∏á: Overlay, Score, Original Image, AI Reconstructed Image\n",
    "    \"\"\"\n",
    "    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û\n",
    "    input_tensor, original_img_np = process_image(img_path)\n",
    "    if input_tensor is None: return None, 0.0, None, None\n",
    "\n",
    "    # --- üîí ‡∏•‡πá‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡∏™‡∏∏‡πà‡∏° (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡πà‡∏á) ---\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # 2. ‡πÉ‡∏™‡πà Noise \n",
    "    noise = torch.randn(input_tensor.shape).to(device)\n",
    "    timesteps = torch.tensor([start_timestep], device=device).long()\n",
    "    noisy_image = scheduler.add_noise(input_tensor, noise, timesteps)\n",
    "\n",
    "    # 3. ‡∏ã‡πà‡∏≠‡∏°‡∏†‡∏≤‡∏û\n",
    "    current_image = noisy_image\n",
    "    scheduler_timesteps = scheduler.timesteps\n",
    "    start_index = (scheduler_timesteps == start_timestep).nonzero(as_tuple=True)[0].item()\n",
    "    subset_timesteps = scheduler_timesteps[start_index:]\n",
    "\n",
    "    for t in subset_timesteps:\n",
    "        with torch.no_grad():\n",
    "            model_output = model(current_image, t).sample\n",
    "            current_image = scheduler.step(model_output, t, current_image).prev_sample\n",
    "\n",
    "    # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û\n",
    "    img_recon = current_image.cpu().numpy()[0, 0]\n",
    "    img_orig = input_tensor.cpu().numpy()[0, 0]\n",
    "    \n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô 0-1\n",
    "    img_orig = (img_orig + 1) / 2\n",
    "    img_recon = (img_recon - img_recon.min()) / (img_recon.max() - img_recon.min())\n",
    "    \n",
    "    # Post-process\n",
    "    img_orig_blur = cv2.GaussianBlur(img_orig, (3, 3), 0)\n",
    "    diff_clean = np.abs(img_orig_blur - img_recon)\n",
    "    \n",
    "    # Clean Background & Ribs\n",
    "    background_mask = np.where(img_orig < 0.1, 0, 1) \n",
    "    diff_clean = diff_clean * background_mask \n",
    "    lesion_map = np.where(diff_clean > threshold, diff_clean, 0)\n",
    "\n",
    "    # 5. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô\n",
    "    lesion_pixels = np.count_nonzero(lesion_map)\n",
    "    lung_pixels = np.count_nonzero(img_orig > 0.1)\n",
    "    \n",
    "    if lung_pixels == 0: score = 0.0\n",
    "    else: score = (lesion_pixels / lung_pixels) * 100\n",
    "\n",
    "    # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û Overlay\n",
    "    heatmap_display = np.clip(lesion_map * 3, 0, 1)\n",
    "    heatmap_color = cv2.applyColorMap((heatmap_display * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    orig_bgr = cv2.cvtColor((img_orig * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    mask_lesion = (heatmap_display > 0).astype(np.uint8) * 255\n",
    "    bg = cv2.bitwise_and(orig_bgr, orig_bgr, mask=cv2.bitwise_not(mask_lesion))\n",
    "    fg = cv2.bitwise_and(heatmap_color, heatmap_color, mask=mask_lesion)\n",
    "    overlay = cv2.add(bg, fg)\n",
    "\n",
    "    # ‚ö° ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏û AI ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢\n",
    "    return overlay, score, img_orig, img_recon\n",
    "\n",
    "# ================= 5. COMPARISON FUNCTION (FROM YOUR CODE - ADAPTED FOR LOOP) =================\n",
    "def generate_comparison_plot(res1, res2, pid, cls_name, dates):\n",
    "    # Unpack Data\n",
    "    overlay1, score1, orig1, recon1 = res1\n",
    "    overlay2, score2, orig2, recon2 = res2\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•\n",
    "    diff_score = score2 - score1\n",
    "    if diff_score < -1.0:\n",
    "        status = \"‚úÖ ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô (Improved)\"\n",
    "        color = 'green'\n",
    "    elif diff_score > 1.0:\n",
    "        status = \"‚ùå ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà‡∏•‡∏á (Worsened)\"\n",
    "        color = 'red'\n",
    "    else:\n",
    "        status = \"‚öñÔ∏è ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏á‡∏ï‡∏±‡∏ß (Stable)\"\n",
    "        color = 'blue'\n",
    "\n",
    "    # ================= üñºÔ∏è ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á 2x3 =================\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # --- ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 1: Day 1 ---\n",
    "    axes[0, 0].imshow(orig1, cmap='gray')\n",
    "    axes[0, 0].set_title(f\"Day 1 ({dates[0]}): Original\", fontsize=12)\n",
    "    \n",
    "    axes[0, 1].imshow(recon1, cmap='gray')\n",
    "    axes[0, 1].set_title(\"Day 1: AI Healed\", fontsize=12)\n",
    "    \n",
    "    axes[0, 2].imshow(cv2.cvtColor(overlay1, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 2].set_title(f\"Day 1 Result: {score1:.2f}%\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    # --- ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 2: Day 2 ---\n",
    "    axes[1, 0].imshow(orig2, cmap='gray')\n",
    "    axes[1, 0].set_title(f\"Day 2 ({dates[1]}): Original\", fontsize=12)\n",
    "    \n",
    "    axes[1, 1].imshow(recon2, cmap='gray')\n",
    "    axes[1, 1].set_title(\"Day 2: AI Healed\", fontsize=12)\n",
    "    \n",
    "    axes[1, 2].imshow(cv2.cvtColor(overlay2, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 2].set_title(f\"Day 2 Result: {score2:.2f}%\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    for ax in axes.flat: ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Patient {pid} Progress: {status} ({diff_score:+.2f}%)\", fontsize=16, color=color, y=0.96)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save Plot\n",
    "    save_path = os.path.join(Config.DIRS[\"comparisons\"], f\"{cls_name}_{pid}_Compare.jpg\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ================= 6. EXECUTION LOOP (Bestpy Copy 2 Structure) =================\n",
    "def get_timeline_files(folder):\n",
    "    files = glob.glob(os.path.join(folder, \"*.jpg\"))\n",
    "    daily = {}\n",
    "    for f in files:\n",
    "        try:\n",
    "            parts = os.path.basename(f).replace(\".jpg\", \"\").split(\"_\")\n",
    "            d = datetime.strptime(parts[-2], \"%Y%m%d\")\n",
    "            s = int(parts[-1])\n",
    "            if d not in daily or s > daily[d][0]: daily[d] = (s, f)\n",
    "        except: continue\n",
    "    return [daily[d][1] for d in sorted(daily)], sorted(daily)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "all_data = []\n",
    "print(\"üöÄ Starting Analysis with YOUR Custom Function...\")\n",
    "\n",
    "for cls in [\"novap\", \"vap\"]:\n",
    "    c_path = os.path.join(Config.ROOT_DIR, cls)\n",
    "    if not os.path.exists(c_path): continue\n",
    "    \n",
    "    for pid in os.listdir(c_path):\n",
    "        p_path = os.path.join(c_path, pid)\n",
    "        files, dates = get_timeline_files(p_path)\n",
    "        if not files: continue\n",
    "        \n",
    "        print(f\"Processing: {pid} ({len(files)} scans)\")\n",
    "        patient_results = [] # Store raw results (overlay, score, orig, recon)\n",
    "        patient_dates = []\n",
    "        \n",
    "        # 1. Analyze Each Image\n",
    "        for i, f in enumerate(files):\n",
    "            date_s = dates[i].strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            # Call YOUR function\n",
    "            res = analyze_lesion_score(f, diffusion_model, noise_scheduler, \n",
    "                                       start_timestep=Config.START_TIMESTEP, \n",
    "                                       threshold=Config.THRESHOLD,\n",
    "                                       seed=Config.SEED)\n",
    "            \n",
    "            overlay, score, orig, recon = res\n",
    "            \n",
    "            if overlay is not None:\n",
    "                patient_results.append(res)\n",
    "                patient_dates.append(date_s)\n",
    "                \n",
    "                # Save Heatmap\n",
    "                fname = f\"{pid}_{date_s}.jpg\"\n",
    "                save_dir = os.path.join(Config.DIRS[\"heatmaps\"], cls, pid)\n",
    "                if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "                cv2.imwrite(os.path.join(save_dir, fname), overlay)\n",
    "                \n",
    "                # Collect Data for CSV\n",
    "                all_data.append({\n",
    "                    \"Patient_ID\": pid,\n",
    "                    \"Date\": date_s,\n",
    "                    \"Percentage of opacity\": round(score, 2),\n",
    "                    \"Group\": cls\n",
    "                })\n",
    "\n",
    "        # 2. Compare First vs Last (using YOUR comparison logic)\n",
    "        if len(patient_results) >= 2:\n",
    "            generate_comparison_plot(patient_results[0], patient_results[-1], pid, cls, [patient_dates[0], patient_dates[-1]])\n",
    "        \n",
    "        # 3. Timeline Plot (Optional - from copy 2)\n",
    "        if len(patient_results) > 0:\n",
    "            scores = [d[\"Percentage of opacity\"] for d in all_data if d[\"Patient_ID\"] == pid]\n",
    "            \n",
    "            plt.figure(figsize=(4 * len(patient_results), 8))\n",
    "            for i in range(len(patient_results)):\n",
    "                ax = plt.subplot(2, len(patient_results), i+1)\n",
    "                ax.imshow(cv2.cvtColor(patient_results[i][0], cv2.COLOR_BGR2RGB)) # [0] is overlay\n",
    "                color = 'red' if scores[i] > 20 else 'green'\n",
    "                ax.set_title(f\"{patient_dates[i]}\\nOp: {scores[i]}%\", color=color, fontweight='bold')\n",
    "                ax.axis('off')\n",
    "            ax2 = plt.subplot(2, 1, 2)\n",
    "            ax2.plot(patient_dates, scores, 'b-o', linewidth=2)\n",
    "            ax2.set_ylim(0, 100); ax2.grid(True, alpha=0.3)\n",
    "            plt.savefig(os.path.join(Config.DIRS[\"timelines\"], f\"{cls}_{pid}_Timeline.jpg\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "# Save CSV\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    csv_path = os.path.join(Config.DIRS[\"csv\"], \"Clinical_Data.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n‚úÖ Final Report Saved: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a411ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Models...\n",
      "‚úÖ Models Ready!\n",
      "üöÄ Starting Analysis with HEX #800000 BACKGROUND outside mask...\n",
      "Processing: 2268 (10 scans)\n",
      "Processing: 2272 (18 scans)\n",
      "Processing: 2273 (4 scans)\n",
      "Processing: 2278 (5 scans)\n",
      "Processing: 2279 (6 scans)\n",
      "Processing: 2283 (6 scans)\n",
      "Processing: 2285 (5 scans)\n",
      "Processing: 2286 (9 scans)\n",
      "Processing: 2288 (5 scans)\n",
      "Processing: 2289 (6 scans)\n",
      "Processing: 2290 (13 scans)\n",
      "Processing: 2299 (2 scans)\n",
      "Processing: 2300 (10 scans)\n",
      "Processing: 2303 (3 scans)\n",
      "Processing: 2304 (6 scans)\n",
      "Processing: 2310 (1 scans)\n",
      "Processing: 2312 (5 scans)\n",
      "Processing: 2315 (5 scans)\n",
      "Processing: 2316 (2 scans)\n",
      "Processing: 2317 (7 scans)\n",
      "Processing: 2320 (2 scans)\n",
      "Processing: 2321 (9 scans)\n",
      "Processing: 2328 (8 scans)\n",
      "Processing: 2336 (6 scans)\n",
      "Processing: 2337 (4 scans)\n",
      "Processing: 2338 (4 scans)\n",
      "Processing: 2131 (7 scans)\n",
      "Processing: 2140 (25 scans)\n",
      "Processing: 2157 (18 scans)\n",
      "Processing: 2167 (14 scans)\n",
      "Processing: 2179 (26 scans)\n",
      "Processing: 2199 (14 scans)\n",
      "Processing: 2200 (13 scans)\n",
      "Processing: 2203 (3 scans)\n",
      "Processing: 2214 (9 scans)\n",
      "Processing: 2219 (11 scans)\n",
      "Processing: 2226 (10 scans)\n",
      "Processing: 2230 (17 scans)\n",
      "Processing: 2231 (13 scans)\n",
      "Processing: 2241 (7 scans)\n",
      "Processing: 2266 (4 scans)\n",
      "Processing: 2277 (9 scans)\n",
      "Processing: 2287 (2 scans)\n",
      "Processing: 2291 (8 scans)\n",
      "Processing: 2292 (3 scans)\n",
      "Processing: 2306 (19 scans)\n",
      "Processing: 2319 (10 scans)\n",
      "Processing: 2332 (19 scans)\n",
      "Processing: 2345 (16 scans)\n",
      "Processing: 2359 (12 scans)\n",
      "Processing: 2380 (13 scans)\n",
      "Processing: 2404 (7 scans)\n",
      "\n",
      "‚úÖ Final Report Saved: Final_User_Code_MaroonBG_800000\\Clinical_Data\\Clinical_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.family'] = 'Tahoma'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================= 1. CONFIGURATION =================\n",
    "class Config:\n",
    "    ROOT_DIR = \"Chest xray CP class\"\n",
    "    MODEL_PATH = \"lung_diffusion_model\"\n",
    "    \n",
    "    # ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ Folder output ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
    "    BASE_OUTPUT = \"Final_User_Code_MaroonBG_800000\" \n",
    "    DIRS = {\n",
    "        \"heatmaps\": os.path.join(BASE_OUTPUT, \"Heatmaps\"),\n",
    "        \"timelines\": os.path.join(BASE_OUTPUT, \"Timelines\"),\n",
    "        \"comparisons\": os.path.join(BASE_OUTPUT, \"Comparisons\"),\n",
    "        \"csv\": os.path.join(BASE_OUTPUT, \"Clinical_Data\")\n",
    "    }\n",
    "    \n",
    "    IMG_SIZE = 128\n",
    "    START_TIMESTEP = 180 \n",
    "    THRESHOLD = 0.15 \n",
    "    SEED = 42\n",
    "\n",
    "for d in Config.DIRS.values():\n",
    "    if not os.path.exists(d): os.makedirs(d)\n",
    "\n",
    "# ================= 2. MODEL LOADING =================\n",
    "print(\"‚è≥ Loading Models...\")\n",
    "seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
    "seg_model.eval()\n",
    "\n",
    "if os.path.exists(Config.MODEL_PATH):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    diffusion_model = UNet2DModel.from_pretrained(Config.MODEL_PATH).to(device)\n",
    "    diffusion_model.eval()\n",
    "    print(\"‚úÖ Models Ready!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Diffusion\")\n",
    "\n",
    "# ================= 3. HELPER FUNCTIONS (Mask & Enhance) =================\n",
    "def get_masks(img_numpy_uint8):\n",
    "    try:\n",
    "        h, w = img_numpy_uint8.shape\n",
    "        img_norm = xrv.datasets.normalize(img_numpy_uint8, 255) \n",
    "        img_resized = cv2.resize(img_norm, (512, 512))\n",
    "        img_tensor = torch.from_numpy(img_resized)[None, None, ...].float()\n",
    "        with torch.no_grad(): outputs = seg_model(img_tensor)\n",
    "        pred = outputs[0].numpy()\n",
    "        def resize_m(m): return cv2.resize(m, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        mask_l = (resize_m(pred[4]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_r = (resize_m(pred[5]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_total = cv2.bitwise_or(mask_l, mask_r)\n",
    "        \n",
    "        return mask_total, mask_l, mask_r\n",
    "    except:\n",
    "        z = np.zeros(img_numpy_uint8.shape, dtype=np.uint8)\n",
    "        return z, z, z\n",
    "\n",
    "def enhance_lung_clarity(img_gray, lung_mask):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(img_gray)\n",
    "    return cv2.bitwise_and(enhanced, enhanced, mask=lung_mask)\n",
    "\n",
    "def process_image(img_path):\n",
    "    if not os.path.exists(img_path): return None, None, None\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: return None, None, None\n",
    "    \n",
    "    mask_total, _, _ = get_masks(img)\n",
    "    \n",
    "    mask_resized = cv2.resize(mask_total, (Config.IMG_SIZE, Config.IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "    mask_binary = (mask_resized > 127).astype(np.float32) \n",
    "\n",
    "    if np.sum(mask_total) == 0: enhanced = img\n",
    "    else: enhanced = enhance_lung_clarity(img, mask_total)\n",
    "        \n",
    "    resized = cv2.resize(enhanced, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "    img_tensor = torch.tensor(resized).float() / 255.0\n",
    "    \n",
    "    return img_tensor.unsqueeze(0).unsqueeze(0).to(device), img, mask_binary\n",
    "\n",
    "# ================= 4. CORE FUNCTION (UPDATED: HEX #800000 BG) =================\n",
    "def analyze_lesion_score(img_path, model, scheduler, start_timestep=180, threshold=0.15, seed=42):\n",
    "    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞ Mask\n",
    "    input_tensor, original_img_np, mask_binary = process_image(img_path)\n",
    "    if input_tensor is None: return None, 0.0, None, None\n",
    "\n",
    "    # --- üîí ‡∏•‡πá‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡∏™‡∏∏‡πà‡∏° ---\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # 2. ‡πÉ‡∏™‡πà Noise \n",
    "    noise = torch.randn(input_tensor.shape).to(device)\n",
    "    timesteps = torch.tensor([start_timestep], device=device).long()\n",
    "    noisy_image = scheduler.add_noise(input_tensor, noise, timesteps)\n",
    "\n",
    "    # 3. ‡∏ã‡πà‡∏≠‡∏°‡∏†‡∏≤‡∏û\n",
    "    current_image = noisy_image\n",
    "    scheduler_timesteps = scheduler.timesteps\n",
    "    start_index = (scheduler_timesteps == start_timestep).nonzero(as_tuple=True)[0].item()\n",
    "    subset_timesteps = scheduler_timesteps[start_index:]\n",
    "\n",
    "    for t in subset_timesteps:\n",
    "        with torch.no_grad():\n",
    "            model_output = model(current_image, t).sample\n",
    "            current_image = scheduler.step(model_output, t, current_image).prev_sample\n",
    "\n",
    "    # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û\n",
    "    img_recon = current_image.cpu().numpy()[0, 0]\n",
    "    img_orig = input_tensor.cpu().numpy()[0, 0]\n",
    "    \n",
    "    img_orig = (img_orig + 1) / 2\n",
    "    img_recon = (img_recon - img_recon.min()) / (img_recon.max() - img_recon.min())\n",
    "    \n",
    "    img_orig_blur = cv2.GaussianBlur(img_orig, (3, 3), 0)\n",
    "    diff_raw = np.abs(img_orig_blur - img_recon)\n",
    "    \n",
    "    # Scoring (MAE inside mask)\n",
    "    diff_inside_lung = diff_raw * mask_binary\n",
    "    lung_pixel_count = np.sum(mask_binary)\n",
    "    if lung_pixel_count == 0: score = 0.0\n",
    "    else: score = (np.sum(diff_inside_lung) / lung_pixel_count) * 100\n",
    "\n",
    "    lesion_map = np.where(diff_inside_lung > threshold, diff_inside_lung, 0)\n",
    "\n",
    "    # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û Overlay\n",
    "    heatmap_display = np.clip(lesion_map * 3, 0, 1)\n",
    "    heatmap_color = cv2.applyColorMap((heatmap_display * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    orig_bgr = cv2.cvtColor((img_orig * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    mask_lesion = (heatmap_display > 0).astype(np.uint8) * 255\n",
    "    bg = cv2.bitwise_and(orig_bgr, orig_bgr, mask=cv2.bitwise_not(mask_lesion))\n",
    "    fg = cv2.bitwise_and(heatmap_color, heatmap_color, mask=mask_lesion)\n",
    "    overlay = cv2.add(bg, fg)\n",
    "\n",
    "    # --- üî¥üî¥ ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏ñ‡∏°‡∏™‡∏µ‡πÅ‡∏î‡∏á‡πÄ‡∏Ç‡πâ‡∏° (#800000) ‡∏ô‡∏≠‡∏Å Mask üî¥üî¥ ---\n",
    "    # mask_binary ‡∏Ñ‡∏∑‡∏≠ 1.0 ‡πÉ‡∏ô‡∏õ‡∏≠‡∏î, 0.0 ‡∏ô‡∏≠‡∏Å‡∏õ‡∏≠‡∏î\n",
    "    outside_mask = (mask_binary == 0)\n",
    "    \n",
    "    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏ô‡∏≠‡∏Å‡∏õ‡∏≠‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏µ‡∏ï‡∏≤‡∏° HEX #800000\n",
    "    # OpenCV ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö BGR ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô #800000 (RGB) ‡∏Ñ‡∏∑‡∏≠ B=0, G=0, R=128 (0x80 ‡∏ê‡∏≤‡∏ô‡∏™‡∏¥‡∏ö‡∏Ñ‡∏∑‡∏≠ 128)\n",
    "    overlay[outside_mask, 0] = 0   # Blue Channel\n",
    "    overlay[outside_mask, 1] = 0   # Green Channel\n",
    "    overlay[outside_mask, 2] = 128 # Red Channel (128 decimal = 80 hex)\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    return overlay, score, img_orig, img_recon\n",
    "\n",
    "# ================= 5. COMPARISON FUNCTION =================\n",
    "def generate_comparison_plot(res1, res2, pid, cls_name, dates):\n",
    "    overlay1, score1, orig1, recon1 = res1\n",
    "    overlay2, score2, orig2, recon2 = res2\n",
    "    \n",
    "    diff_score = score2 - score1\n",
    "    \n",
    "    if diff_score < 0: status = \"‚úÖ ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô (Improved)\"; color = 'green'\n",
    "    elif diff_score > 0: status = \"‚ùå ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà‡∏•‡∏á (Worsened)\"; color = 'red'\n",
    "    else: status = \"‚öñÔ∏è ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏á‡∏ï‡∏±‡∏ß (Stable)\"; color = 'blue'\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Day 1\n",
    "    axes[0, 0].imshow(orig1, cmap='gray'); axes[0, 0].set_title(f\"Day 1 ({dates[0]})\", fontsize=12)\n",
    "    axes[0, 1].imshow(recon1, cmap='gray'); axes[0, 1].set_title(\"Day 1: AI Healed\", fontsize=12)\n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û Overlay (‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏î‡∏á‡πÄ‡∏Ç‡πâ‡∏° #800000)\n",
    "    axes[0, 2].imshow(cv2.cvtColor(overlay1, cv2.COLOR_BGR2RGB)) \n",
    "    axes[0, 2].set_title(f\"Score: {score1:.2f} (MAE)\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    # Day 2\n",
    "    axes[1, 0].imshow(orig2, cmap='gray'); axes[1, 0].set_title(f\"Day 2 ({dates[1]})\", fontsize=12)\n",
    "    axes[1, 1].imshow(recon2, cmap='gray'); axes[1, 1].set_title(\"Day 2: AI Healed\", fontsize=12)\n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û Overlay (‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏î‡∏á‡πÄ‡∏Ç‡πâ‡∏° #800000)\n",
    "    axes[1, 2].imshow(cv2.cvtColor(overlay2, cv2.COLOR_BGR2RGB)) \n",
    "    axes[1, 2].set_title(f\"Score: {score2:.2f} (MAE)\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    for ax in axes.flat: ax.axis('off')\n",
    "    plt.suptitle(f\"Patient {pid} Progress: {status} (Diff: {diff_score:+.2f})\", fontsize=16, color=color, y=0.96)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(Config.DIRS[\"comparisons\"], f\"{cls_name}_{pid}_Compare.jpg\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ================= 6. EXECUTION LOOP =================\n",
    "def get_timeline_files(folder):\n",
    "    files = glob.glob(os.path.join(folder, \"*.jpg\"))\n",
    "    daily = {}\n",
    "    for f in files:\n",
    "        try:\n",
    "            parts = os.path.basename(f).replace(\".jpg\", \"\").split(\"_\")\n",
    "            d = datetime.strptime(parts[-2], \"%Y%m%d\")\n",
    "            s = int(parts[-1])\n",
    "            if d not in daily or s > daily[d][0]: daily[d] = (s, f)\n",
    "        except: continue\n",
    "    return [daily[d][1] for d in sorted(daily)], sorted(daily)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "all_data = []\n",
    "print(\"üöÄ Starting Analysis with HEX #800000 BACKGROUND outside mask...\")\n",
    "\n",
    "for cls in [\"novap\", \"vap\"]:\n",
    "    c_path = os.path.join(Config.ROOT_DIR, cls)\n",
    "    if not os.path.exists(c_path): continue\n",
    "    \n",
    "    for pid in os.listdir(c_path):\n",
    "        p_path = os.path.join(c_path, pid)\n",
    "        files, dates = get_timeline_files(p_path)\n",
    "        if not files: continue\n",
    "        \n",
    "        print(f\"Processing: {pid} ({len(files)} scans)\")\n",
    "        patient_results = []\n",
    "        patient_dates = []\n",
    "        \n",
    "        # 1. Analyze Each Image\n",
    "        for i, f in enumerate(files):\n",
    "            date_s = dates[i].strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            res = analyze_lesion_score(f, diffusion_model, noise_scheduler, \n",
    "                                       start_timestep=Config.START_TIMESTEP, \n",
    "                                       threshold=Config.THRESHOLD,\n",
    "                                       seed=Config.SEED)\n",
    "            \n",
    "            overlay, score, orig, recon = res\n",
    "            \n",
    "            if overlay is not None:\n",
    "                patient_results.append(res)\n",
    "                patient_dates.append(date_s)\n",
    "                \n",
    "                # Save Heatmap\n",
    "                fname = f\"{pid}_{date_s}.jpg\"\n",
    "                save_dir = os.path.join(Config.DIRS[\"heatmaps\"], cls, pid)\n",
    "                if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "                cv2.imwrite(os.path.join(save_dir, fname), overlay)\n",
    "                \n",
    "                all_data.append({\n",
    "                    \"Patient_ID\": pid,\n",
    "                    \"Date\": date_s,\n",
    "                    \"Opacity_Score_MAE\": round(score, 2),\n",
    "                    \"Group\": cls\n",
    "                })\n",
    "\n",
    "        # 2. Compare First vs Last\n",
    "        if len(patient_results) >= 2:\n",
    "            generate_comparison_plot(patient_results[0], patient_results[-1], pid, cls, [patient_dates[0], patient_dates[-1]])\n",
    "        \n",
    "        # 3. Timeline Plot\n",
    "        if len(patient_results) > 0:\n",
    "            scores = [d[\"Opacity_Score_MAE\"] for d in all_data if d[\"Patient_ID\"] == pid]\n",
    "            \n",
    "            plt.figure(figsize=(4 * len(patient_results), 8))\n",
    "            for i in range(len(patient_results)):\n",
    "                ax = plt.subplot(2, len(patient_results), i+1)\n",
    "                # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û Overlay (‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏î‡∏á‡πÄ‡∏Ç‡πâ‡∏° #800000)\n",
    "                ax.imshow(cv2.cvtColor(patient_results[i][0], cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                current_score = scores[i]\n",
    "                if i == 0: color = 'blue'\n",
    "                else:\n",
    "                    prev_score = scores[i-1]\n",
    "                    if current_score < prev_score: color = 'green'\n",
    "                    elif current_score > prev_score: color = 'red'\n",
    "                    else: color = 'blue'\n",
    "                \n",
    "                ax.set_title(f\"{patient_dates[i]}\\nScore: {current_score}\", color=color, fontweight='bold', fontsize=14)\n",
    "                ax.axis('off')\n",
    "            \n",
    "            ax2 = plt.subplot(2, 1, 2)\n",
    "            ax2.plot(patient_dates, scores, 'b-o', linewidth=2)\n",
    "            ax2.set_ylabel(\"Opacity Score (MAE)\")\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.suptitle(f\"Timeline: {pid} ({cls})\", fontsize=16)\n",
    "            plt.savefig(os.path.join(Config.DIRS[\"timelines\"], f\"{cls}_{pid}_Timeline.jpg\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "# Save CSV\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    csv_path = os.path.join(Config.DIRS[\"csv\"], \"Clinical_Data.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n‚úÖ Final Report Saved: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Models...\n",
      "‚úÖ Models Ready!\n",
      "üöÄ Starting Analysis with FULL CSV METRICS...\n",
      "Processing: 2268 (10 scans)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.family'] = 'Tahoma'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================= 1. CONFIGURATION =================\n",
    "class Config:\n",
    "    ROOT_DIR = \"Chest xray CP class\"\n",
    "    MODEL_PATH = \"lung_diffusion_model\"\n",
    "    \n",
    "    BASE_OUTPUT = \"Final_User_Code_FullMetrics_CSV\" \n",
    "    DIRS = {\n",
    "        \"heatmaps\": os.path.join(BASE_OUTPUT, \"Heatmaps\"),\n",
    "        \"timelines\": os.path.join(BASE_OUTPUT, \"Timelines\"),\n",
    "        \"comparisons\": os.path.join(BASE_OUTPUT, \"Comparisons\"),\n",
    "        \"csv\": os.path.join(BASE_OUTPUT, \"Clinical_Data\")\n",
    "    }\n",
    "    \n",
    "    IMG_SIZE = 128\n",
    "    START_TIMESTEP = 180 \n",
    "    THRESHOLD = 0.15 \n",
    "    HIGH_OPACITY_THRESHOLD = 0.40 # ‡∏Ñ‡πà‡∏≤‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö High Opacity ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô Consolidation)\n",
    "    SEED = 42\n",
    "\n",
    "for d in Config.DIRS.values():\n",
    "    if not os.path.exists(d): os.makedirs(d)\n",
    "\n",
    "# ================= 2. MODEL LOADING =================\n",
    "print(\"‚è≥ Loading Models...\")\n",
    "seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
    "seg_model.eval()\n",
    "\n",
    "if os.path.exists(Config.MODEL_PATH):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    diffusion_model = UNet2DModel.from_pretrained(Config.MODEL_PATH).to(device)\n",
    "    diffusion_model.eval()\n",
    "    print(\"‚úÖ Models Ready!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Diffusion\")\n",
    "\n",
    "# ================= 3. HELPER FUNCTIONS =================\n",
    "def get_masks(img_numpy_uint8):\n",
    "    try:\n",
    "        h, w = img_numpy_uint8.shape\n",
    "        img_norm = xrv.datasets.normalize(img_numpy_uint8, 255) \n",
    "        img_resized = cv2.resize(img_norm, (512, 512))\n",
    "        img_tensor = torch.from_numpy(img_resized)[None, None, ...].float()\n",
    "        with torch.no_grad(): outputs = seg_model(img_tensor)\n",
    "        pred = outputs[0].numpy()\n",
    "        def resize_m(m): return cv2.resize(m, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Mask ‡πÅ‡∏¢‡∏Å‡∏ã‡πâ‡∏≤‡∏¢‡∏Ç‡∏ß‡∏≤\n",
    "        mask_l = (resize_m(pred[4]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_r = (resize_m(pred[5]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_total = cv2.bitwise_or(mask_l, mask_r)\n",
    "        \n",
    "        return mask_total, mask_l, mask_r\n",
    "    except:\n",
    "        z = np.zeros(img_numpy_uint8.shape, dtype=np.uint8)\n",
    "        return z, z, z\n",
    "\n",
    "def enhance_lung_clarity(img_gray, lung_mask):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(img_gray)\n",
    "    return cv2.bitwise_and(enhanced, enhanced, mask=lung_mask)\n",
    "\n",
    "def process_image(img_path):\n",
    "    if not os.path.exists(img_path): return None, None, None, None, None\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: return None, None, None, None, None\n",
    "    \n",
    "    # ‡πÑ‡∏î‡πâ Mask ‡∏£‡∏ß‡∏°, Mask ‡∏ã‡πâ‡∏≤‡∏¢, Mask ‡∏Ç‡∏ß‡∏≤\n",
    "    mask_total, mask_l, mask_r = get_masks(img)\n",
    "    \n",
    "    # Resize Masks ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö IMG_SIZE (128)\n",
    "    def resize_binary(m):\n",
    "        m_res = cv2.resize(m, (Config.IMG_SIZE, Config.IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "        return (m_res > 127).astype(np.float32)\n",
    "\n",
    "    mask_binary_total = resize_binary(mask_total)\n",
    "    mask_binary_l = resize_binary(mask_l)\n",
    "    mask_binary_r = resize_binary(mask_r)\n",
    "\n",
    "    if np.sum(mask_total) == 0: enhanced = img\n",
    "    else: enhanced = enhance_lung_clarity(img, mask_total)\n",
    "        \n",
    "    resized = cv2.resize(enhanced, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "    img_tensor = torch.tensor(resized).float() / 255.0\n",
    "    \n",
    "    return img_tensor.unsqueeze(0).unsqueeze(0).to(device), img, mask_binary_total, mask_binary_l, mask_binary_r\n",
    "\n",
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Stat ‡∏£‡∏≤‡∏¢‡∏õ‡∏≠‡∏î (Mapping CT metrics -> X-ray metrics)\n",
    "def calculate_lung_metrics(diff_map, mask_binary, img_orig_norm):\n",
    "    \"\"\"\n",
    "    diff_map: ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ (Difference Map)\n",
    "    mask_binary: ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏≠‡∏î (0 ‡∏´‡∏£‡∏∑‡∏≠ 1)\n",
    "    img_orig_norm: ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (0.0 - 1.0) ‡πÉ‡∏ä‡πâ‡∏´‡∏≤ Intensity (‡πÅ‡∏ó‡∏ô HU)\n",
    "    \"\"\"\n",
    "    lung_pixels = np.sum(mask_binary)\n",
    "    \n",
    "    if lung_pixels == 0:\n",
    "        return {\n",
    "            \"Affected\": \"No\",\n",
    "            \"Opacity Score\": 0,\n",
    "            \"Lung Volume (px)\": 0,\n",
    "            \"Volume of Opacity (px)\": 0,\n",
    "            \"% Opacity\": 0.0,\n",
    "            \"Volume High Opacity (px)\": 0,\n",
    "            \"% High Opacity\": 0.0,\n",
    "            \"Mean Intensity Total\": 0.0,\n",
    "            \"Mean Intensity Opacity\": 0.0,\n",
    "            \"Std Dev Total\": 0.0,\n",
    "            \"Std Dev Opacity\": 0.0\n",
    "        }\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏õ‡∏≠‡∏î\n",
    "    diff_inside = diff_map * mask_binary\n",
    "    img_inside = img_orig_norm * mask_binary\n",
    "    \n",
    "    # 1. Opacity (Threshold ‡∏õ‡∏Å‡∏ï‡∏¥)\n",
    "    opacity_mask = (diff_inside > Config.THRESHOLD).astype(np.float32)\n",
    "    opacity_pixels = np.sum(opacity_mask)\n",
    "    \n",
    "    # 2. High Opacity (Threshold ‡∏™‡∏π‡∏á)\n",
    "    high_opacity_mask = (diff_inside > Config.HIGH_OPACITY_THRESHOLD).astype(np.float32)\n",
    "    high_opacity_pixels = np.sum(high_opacity_mask)\n",
    "    \n",
    "    # 3. Percentages\n",
    "    pct_opacity = (opacity_pixels / lung_pixels) * 100\n",
    "    pct_high_opacity = (high_opacity_pixels / lung_pixels) * 100\n",
    "    \n",
    "    # 4. Intensity Stats (Mapping HU -> Intensity 0-255)\n",
    "    # Mean Total (‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏≠‡∏î)\n",
    "    mean_intensity_total = np.sum(img_inside) / lung_pixels * 255\n",
    "    \n",
    "    # Mean Opacity (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ù‡πâ‡∏≤)\n",
    "    if opacity_pixels > 0:\n",
    "        mean_intensity_opacity = np.sum(img_inside * opacity_mask) / opacity_pixels * 255\n",
    "    else:\n",
    "        mean_intensity_opacity = 0.0\n",
    "        \n",
    "    # Std Dev Total\n",
    "    # ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì std (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ pixel ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô mask)\n",
    "    pixels_in_lung = img_orig_norm[mask_binary == 1]\n",
    "    std_total = np.std(pixels_in_lung * 255) if len(pixels_in_lung) > 0 else 0\n",
    "    \n",
    "    # Std Dev Opacity\n",
    "    pixels_in_opacity = img_orig_norm[opacity_mask == 1]\n",
    "    std_opacity = np.std(pixels_in_opacity * 255) if len(pixels_in_opacity) > 0 else 0\n",
    "    \n",
    "    # 5. Opacity Score (Simple scaling 0-4 per region, similar to paper but simplified)\n",
    "    # Paper: 0=0%, 1=<25%, 2=<50%, 3=<75%, 4=>75%\n",
    "    if pct_opacity <= 1: op_score = 0\n",
    "    elif pct_opacity <= 25: op_score = 1\n",
    "    elif pct_opacity <= 50: op_score = 2\n",
    "    elif pct_opacity <= 75: op_score = 3\n",
    "    else: op_score = 4\n",
    "\n",
    "    return {\n",
    "        \"Affected\": \"Yes\" if pct_opacity > 1 else \"No\",\n",
    "        \"Opacity Score\": op_score,\n",
    "        \"Lung Volume (px)\": int(lung_pixels),\n",
    "        \"Volume of Opacity (px)\": int(opacity_pixels),\n",
    "        \"% Opacity\": round(pct_opacity, 2),\n",
    "        \"Volume High Opacity (px)\": int(high_opacity_pixels),\n",
    "        \"% High Opacity\": round(pct_high_opacity, 2),\n",
    "        \"Mean Intensity Total\": round(mean_intensity_total, 2),\n",
    "        \"Mean Intensity Opacity\": round(mean_intensity_opacity, 2),\n",
    "        \"Std Dev Total\": round(std_total, 2),\n",
    "        \"Std Dev Opacity\": round(std_opacity, 2)\n",
    "    }\n",
    "\n",
    "# ================= 4. CORE FUNCTION =================\n",
    "def analyze_lesion_score(img_path, model, scheduler, start_timestep=180, threshold=0.15, seed=42):\n",
    "    # ‡∏£‡∏±‡∏ö Mask ‡πÅ‡∏¢‡∏Å‡∏ã‡πâ‡∏≤‡∏¢‡∏Ç‡∏ß‡∏≤‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢\n",
    "    input_tensor, original_img_np, mask_total, mask_l, mask_r = process_image(img_path)\n",
    "    if input_tensor is None: return None, None, None, None, None\n",
    "\n",
    "    # --- Locus ---\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Diffusion Process\n",
    "    noise = torch.randn(input_tensor.shape).to(device)\n",
    "    timesteps = torch.tensor([start_timestep], device=device).long()\n",
    "    noisy_image = scheduler.add_noise(input_tensor, noise, timesteps)\n",
    "\n",
    "    current_image = noisy_image\n",
    "    scheduler_timesteps = scheduler.timesteps\n",
    "    start_index = (scheduler_timesteps == start_timestep).nonzero(as_tuple=True)[0].item()\n",
    "    subset_timesteps = scheduler_timesteps[start_index:]\n",
    "\n",
    "    for t in subset_timesteps:\n",
    "        with torch.no_grad():\n",
    "            model_output = model(current_image, t).sample\n",
    "            current_image = scheduler.step(model_output, t, current_image).prev_sample\n",
    "\n",
    "    # Prepare Images\n",
    "    img_recon = current_image.cpu().numpy()[0, 0]\n",
    "    img_orig = input_tensor.cpu().numpy()[0, 0]\n",
    "    \n",
    "    img_orig = (img_orig + 1) / 2\n",
    "    img_recon = (img_recon - img_recon.min()) / (img_recon.max() - img_recon.min())\n",
    "    \n",
    "    img_orig_blur = cv2.GaussianBlur(img_orig, (3, 3), 0)\n",
    "    diff_raw = np.abs(img_orig_blur - img_recon)\n",
    "    \n",
    "    # --- üìä CALCULATE METRICS (L/R/Total) ---\n",
    "    metrics_total = calculate_lung_metrics(diff_raw, mask_total, img_orig)\n",
    "    metrics_left = calculate_lung_metrics(diff_raw, mask_l, img_orig)\n",
    "    metrics_right = calculate_lung_metrics(diff_raw, mask_r, img_orig)\n",
    "    \n",
    "    # Score ‡∏´‡∏•‡∏±‡∏Å (MAE) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü\n",
    "    lung_pixel_count = metrics_total[\"Lung Volume (px)\"]\n",
    "    if lung_pixel_count == 0: score_mae = 0.0\n",
    "    else: \n",
    "        diff_inside = diff_raw * mask_total\n",
    "        score_mae = (np.sum(diff_inside) / lung_pixel_count) * 100\n",
    "\n",
    "    # --- Overlay Generation ---\n",
    "    diff_inside_lung = diff_raw * mask_total\n",
    "    lesion_map = np.where(diff_inside_lung > threshold, diff_inside_lung, 0)\n",
    "    \n",
    "    heatmap_display = np.clip(lesion_map * 3, 0, 1)\n",
    "    heatmap_color = cv2.applyColorMap((heatmap_display * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    orig_bgr = cv2.cvtColor((img_orig * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    mask_lesion = (heatmap_display > 0).astype(np.uint8) * 255\n",
    "    bg = cv2.bitwise_and(orig_bgr, orig_bgr, mask=cv2.bitwise_not(mask_lesion))\n",
    "    fg = cv2.bitwise_and(heatmap_color, heatmap_color, mask=mask_lesion)\n",
    "    overlay = cv2.add(bg, fg)\n",
    "\n",
    "    # BG ‡∏™‡∏µ‡πÅ‡∏î‡∏á‡πÄ‡∏Ç‡πâ‡∏° #800000\n",
    "    outside_mask = (mask_total == 0)\n",
    "    overlay[outside_mask, 0] = 0\n",
    "    overlay[outside_mask, 1] = 0\n",
    "    overlay[outside_mask, 2] = 128\n",
    "\n",
    "    return overlay, score_mae, img_orig, img_recon, (metrics_total, metrics_left, metrics_right)\n",
    "\n",
    "# ================= 5. COMPARISON FUNCTION =================\n",
    "def generate_comparison_plot(res1, res2, pid, cls_name, dates):\n",
    "    overlay1, score1, orig1, recon1, _ = res1\n",
    "    overlay2, score2, orig2, recon2, _ = res2\n",
    "    \n",
    "    diff_score = score2 - score1\n",
    "    \n",
    "    if diff_score < 0: status = \"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô (Improved)\"; color = 'green'\n",
    "    elif diff_score > 0: status = \"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà‡∏•‡∏á (Worsened)\"; color = 'red'\n",
    "    else: status = \"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏á‡∏ï‡∏±‡∏ß (Stable)\"; color = 'blue'\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Day 1\n",
    "    axes[0, 0].imshow(orig1, cmap='gray'); axes[0, 0].set_title(f\"Day 1 ({dates[0]})\", fontsize=12)\n",
    "    axes[0, 1].imshow(recon1, cmap='gray'); axes[0, 1].set_title(\"Day 1: AI Healed\", fontsize=12)\n",
    "    axes[0, 2].imshow(cv2.cvtColor(overlay1, cv2.COLOR_BGR2RGB)) \n",
    "    axes[0, 2].set_title(f\"Score: {score1:.2f} (MAE)\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    # Day Last\n",
    "    axes[1, 0].imshow(orig2, cmap='gray'); axes[1, 0].set_title(f\"Day Last ({dates[1]})\", fontsize=12)\n",
    "    axes[1, 1].imshow(recon2, cmap='gray'); axes[1, 1].set_title(\"Day Last: AI Healed\", fontsize=12)\n",
    "    axes[1, 2].imshow(cv2.cvtColor(overlay2, cv2.COLOR_BGR2RGB)) \n",
    "    axes[1, 2].set_title(f\"Score: {score2:.2f} (MAE)\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    for ax in axes.flat: ax.axis('off')\n",
    "    plt.suptitle(f\"Patient {pid} Progress: {status} (Diff: {diff_score:+.2f})\", fontsize=16, color=color, y=1)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(Config.DIRS[\"comparisons\"], f\"{cls_name}_{pid}_Compare.jpg\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ================= 6. EXECUTION LOOP =================\n",
    "def get_timeline_files(folder):\n",
    "    files = glob.glob(os.path.join(folder, \"*.jpg\"))\n",
    "    daily = {}\n",
    "    for f in files:\n",
    "        try:\n",
    "            parts = os.path.basename(f).replace(\".jpg\", \"\").split(\"_\")\n",
    "            d = datetime.strptime(parts[-2], \"%Y%m%d\")\n",
    "            s = int(parts[-1])\n",
    "            if d not in daily or s > daily[d][0]: daily[d] = (s, f)\n",
    "        except: continue\n",
    "    return [daily[d][1] for d in sorted(daily)], sorted(daily)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "all_csv_data = [] # List ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\n",
    "print(\"üöÄ Starting Analysis with FULL CSV METRICS...\")\n",
    "\n",
    "for cls in [\"novap\", \"vap\"]:\n",
    "    c_path = os.path.join(Config.ROOT_DIR, cls)\n",
    "    if not os.path.exists(c_path): continue\n",
    "    \n",
    "    for pid in os.listdir(c_path):\n",
    "        p_path = os.path.join(c_path, pid)\n",
    "        files, dates = get_timeline_files(p_path)\n",
    "        if not files: continue\n",
    "        \n",
    "        print(f\"Processing: {pid} ({len(files)} scans)\")\n",
    "        patient_results = []\n",
    "        patient_dates = []\n",
    "        mae_scores = []\n",
    "        \n",
    "        # 1. Analyze Each Image\n",
    "        for i, f in enumerate(files):\n",
    "            date_s = dates[i].strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß\n",
    "            res = analyze_lesion_score(f, diffusion_model, noise_scheduler, \n",
    "                                       start_timestep=Config.START_TIMESTEP, \n",
    "                                       threshold=Config.THRESHOLD,\n",
    "                                       seed=Config.SEED)\n",
    "            \n",
    "            overlay, score_mae, orig, recon, detailed_metrics = res\n",
    "            \n",
    "            if overlay is not None:\n",
    "                patient_results.append(res)\n",
    "                patient_dates.append(date_s)\n",
    "                mae_scores.append(score_mae)\n",
    "                \n",
    "                # Unpack metrics\n",
    "                m_total, m_left, m_right = detailed_metrics\n",
    "                \n",
    "                # Save Heatmap\n",
    "                fname = f\"{pid}_{date_s}.jpg\"\n",
    "                save_dir = os.path.join(Config.DIRS[\"heatmaps\"], cls, pid)\n",
    "                if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "                cv2.imwrite(os.path.join(save_dir, fname), overlay)\n",
    "                \n",
    "                # --- PREPARE CSV ROW DATA ---\n",
    "                row = {\n",
    "                    \"Patient_ID\": pid,\n",
    "                    \"Date\": date_s,\n",
    "                    \"Group\": cls,\n",
    "                    \n",
    "                    # --- Total Overview ---\n",
    "                    \"Total Opacity Score (Sum L+R)\": m_left[\"Opacity Score\"] + m_right[\"Opacity Score\"],\n",
    "                    \"Total % Opacity\": m_total[\"% Opacity\"],\n",
    "                    \n",
    "                    # --- Both Lungs ---\n",
    "                    \"Both_Affected\": m_total[\"Affected\"],\n",
    "                    \"Both_Opacity_Score\": m_total[\"Opacity Score\"], # Score ‡∏£‡∏ß‡∏°‡πÅ‡∏ö‡∏ö MAE mapping\n",
    "                    \"Both_Lung_Volume_px\": m_total[\"Lung Volume (px)\"],\n",
    "                    \"Both_Volume_Opacity_px\": m_total[\"Volume of Opacity (px)\"],\n",
    "                    \"Both_Pct_Opacity\": m_total[\"% Opacity\"],\n",
    "                    \"Both_Volume_High_Opacity_px\": m_total[\"Volume High Opacity (px)\"],\n",
    "                    \"Both_Pct_High_Opacity\": m_total[\"% High Opacity\"],\n",
    "                    \"Both_Mean_Intensity\": m_total[\"Mean Intensity Total\"],\n",
    "                    \"Both_Mean_Intensity_Opacity\": m_total[\"Mean Intensity Opacity\"],\n",
    "                    \"Both_StdDev_Total\": m_total[\"Std Dev Total\"],\n",
    "                    \"Both_StdDev_Opacity\": m_total[\"Std Dev Opacity\"],\n",
    "\n",
    "                    # --- Left Lung ---\n",
    "                    \"Left_Affected\": m_left[\"Affected\"],\n",
    "                    \"Left_Opacity_Score\": m_left[\"Opacity Score\"],\n",
    "                    \"Left_Lung_Volume_px\": m_left[\"Lung Volume (px)\"],\n",
    "                    \"Left_Volume_Opacity_px\": m_left[\"Volume of Opacity (px)\"],\n",
    "                    \"Left_Pct_Opacity\": m_left[\"% Opacity\"],\n",
    "                    \"Left_Volume_High_Opacity_px\": m_left[\"Volume High Opacity (px)\"],\n",
    "                    \"Left_Pct_High_Opacity\": m_left[\"% High Opacity\"],\n",
    "                    \"Left_Mean_Intensity\": m_left[\"Mean Intensity Total\"],\n",
    "                    \"Left_Mean_Intensity_Opacity\": m_left[\"Mean Intensity Opacity\"],\n",
    "                    \"Left_StdDev_Total\": m_left[\"Std Dev Total\"],\n",
    "                    \"Left_StdDev_Opacity\": m_left[\"Std Dev Opacity\"],\n",
    "\n",
    "                    # --- Right Lung ---\n",
    "                    \"Right_Affected\": m_right[\"Affected\"],\n",
    "                    \"Right_Opacity_Score\": m_right[\"Opacity Score\"],\n",
    "                    \"Right_Lung_Volume_px\": m_right[\"Lung Volume (px)\"],\n",
    "                    \"Right_Volume_Opacity_px\": m_right[\"Volume of Opacity (px)\"],\n",
    "                    \"Right_Pct_Opacity\": m_right[\"% Opacity\"],\n",
    "                    \"Right_Volume_High_Opacity_px\": m_right[\"Volume High Opacity (px)\"],\n",
    "                    \"Right_Pct_High_Opacity\": m_right[\"% High Opacity\"],\n",
    "                    \"Right_Mean_Intensity\": m_right[\"Mean Intensity Total\"],\n",
    "                    \"Right_Mean_Intensity_Opacity\": m_right[\"Mean Intensity Opacity\"],\n",
    "                    \"Right_StdDev_Total\": m_right[\"Std Dev Total\"],\n",
    "                    \"Right_StdDev_Opacity\": m_right[\"Std Dev Opacity\"],\n",
    "                }\n",
    "                all_csv_data.append(row)\n",
    "\n",
    "        # 2. Compare First vs Last\n",
    "        if len(patient_results) >= 2:\n",
    "            generate_comparison_plot(patient_results[0], patient_results[-1], pid, cls, [patient_dates[0], patient_dates[-1]])\n",
    "        \n",
    "        # 3. Timeline Plot\n",
    "        if len(patient_results) > 0:\n",
    "            plt.figure(figsize=(4 * len(patient_results), 8))\n",
    "            for i in range(len(patient_results)):\n",
    "                ax = plt.subplot(2, len(patient_results), i+1)\n",
    "                ax.imshow(cv2.cvtColor(patient_results[i][0], cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                current_score = mae_scores[i]\n",
    "                if i == 0: color = 'blue'\n",
    "                else:\n",
    "                    prev_score = mae_scores[i-1]\n",
    "                    if current_score < prev_score: color = 'green'\n",
    "                    elif current_score > prev_score: color = 'red'\n",
    "                    else: color = 'blue'\n",
    "                \n",
    "                ax.set_title(f\"{patient_dates[i]}\\nScore: {current_score:.2f}\", color=color, fontweight='bold', fontsize=14)\n",
    "                ax.axis('off')\n",
    "            \n",
    "            ax2 = plt.subplot(2, 1, 2)\n",
    "            ax2.plot(patient_dates, mae_scores, 'b-o', linewidth=2)\n",
    "            ax2.set_ylabel(\"Opacity Score (MAE)\")\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.suptitle(f\"Timeline: {pid} ({cls})\", fontsize=16)\n",
    "            plt.savefig(os.path.join(Config.DIRS[\"timelines\"], f\"{cls}_{pid}_Timeline.jpg\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "# Save CSV with Full Metrics\n",
    "if all_csv_data:\n",
    "    df = pd.DataFrame(all_csv_data)\n",
    "    \n",
    "    # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á Column ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡∏≠\n",
    "    cols = [\"Patient_ID\", \"Date\", \"Group\", \"Total Opacity Score (Sum L+R)\", \"Total % Opacity\"]\n",
    "    # ‡πÄ‡∏û‡∏¥‡πà‡∏° Column ‡∏Ç‡∏≠‡∏á Both/Left/Right ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢\n",
    "    cols += [c for c in df.columns if c.startswith(\"Both_\")]\n",
    "    cols += [c for c in df.columns if c.startswith(\"Left_\")]\n",
    "    cols += [c for c in df.columns if c.startswith(\"Right_\")]\n",
    "    \n",
    "    df = df[cols]\n",
    "    \n",
    "    csv_path = os.path.join(Config.DIRS[\"csv\"], \"Clinical_Data.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n‚úÖ Final Clinical Report Saved: {csv_path}\")\n",
    "    print(\"   Note: Volume is in 'pixels' and Intensity is '0-255' (mapped from CT methods for X-ray)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b124817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Models...\n",
      "‚úÖ Models Ready!\n",
      "üöÄ Starting Analysis with Detailed CSV & Black Background...\n",
      "Processing: 2268 (10 scans)\n",
      "Processing: 2272 (18 scans)\n",
      "Processing: 2273 (4 scans)\n",
      "Processing: 2278 (5 scans)\n",
      "Processing: 2279 (6 scans)\n",
      "Processing: 2283 (6 scans)\n",
      "Processing: 2285 (5 scans)\n",
      "Processing: 2286 (9 scans)\n",
      "Processing: 2288 (5 scans)\n",
      "Processing: 2289 (6 scans)\n",
      "Processing: 2290 (13 scans)\n",
      "Processing: 2299 (2 scans)\n",
      "Processing: 2300 (10 scans)\n",
      "Processing: 2303 (3 scans)\n",
      "Processing: 2304 (6 scans)\n",
      "Processing: 2310 (1 scans)\n",
      "Processing: 2312 (5 scans)\n",
      "Processing: 2315 (5 scans)\n",
      "Processing: 2316 (2 scans)\n",
      "Processing: 2317 (7 scans)\n",
      "Processing: 2320 (2 scans)\n",
      "Processing: 2321 (9 scans)\n",
      "Processing: 2328 (8 scans)\n",
      "Processing: 2336 (6 scans)\n",
      "Processing: 2337 (4 scans)\n",
      "Processing: 2338 (4 scans)\n",
      "Processing: 2131 (7 scans)\n",
      "Processing: 2140 (25 scans)\n",
      "Processing: 2157 (18 scans)\n",
      "Processing: 2167 (14 scans)\n",
      "Processing: 2179 (26 scans)\n",
      "Processing: 2199 (14 scans)\n",
      "Processing: 2200 (13 scans)\n",
      "Processing: 2203 (3 scans)\n",
      "Processing: 2214 (9 scans)\n",
      "Processing: 2219 (11 scans)\n",
      "Processing: 2226 (10 scans)\n",
      "Processing: 2230 (17 scans)\n",
      "Processing: 2231 (13 scans)\n",
      "Processing: 2241 (7 scans)\n",
      "Processing: 2266 (4 scans)\n",
      "Processing: 2277 (9 scans)\n",
      "Processing: 2287 (2 scans)\n",
      "Processing: 2291 (8 scans)\n",
      "Processing: 2292 (3 scans)\n",
      "Processing: 2306 (19 scans)\n",
      "Processing: 2319 (10 scans)\n",
      "Processing: 2332 (19 scans)\n",
      "Processing: 2345 (16 scans)\n",
      "Processing: 2359 (12 scans)\n",
      "Processing: 2380 (13 scans)\n",
      "Processing: 2404 (7 scans)\n",
      "\n",
      "‚úÖ Final Clinical Report Saved: Final_Paper_Based_Heatmap\\Clinical_Data\\Clinical_Data_Detailed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.family'] = 'Tahoma'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================= 1. CONFIGURATION =================\n",
    "class Config:\n",
    "    ROOT_DIR = \"Chest xray CP class\"\n",
    "    MODEL_PATH = \"lung_diffusion_model\"\n",
    "    \n",
    "    BASE_OUTPUT = \"Final_Paper_Based_Heatmap\" \n",
    "    DIRS = {\n",
    "        \"heatmaps\": os.path.join(BASE_OUTPUT, \"Heatmaps\"),\n",
    "        \"timelines\": os.path.join(BASE_OUTPUT, \"Timelines\"),\n",
    "        \"comparisons\": os.path.join(BASE_OUTPUT, \"Comparisons\"),\n",
    "        \"csv\": os.path.join(BASE_OUTPUT, \"Clinical_Data\")\n",
    "    }\n",
    "    \n",
    "    IMG_SIZE = 128\n",
    "    START_TIMESTEP = 180 \n",
    "    \n",
    "    # Thresholds\n",
    "    HEATMAP_THRESHOLD = 0.10 \n",
    "    CONTOUR_THRESHOLD = 0.25 \n",
    "    \n",
    "    SEED = 42\n",
    "\n",
    "for d in Config.DIRS.values():\n",
    "    if not os.path.exists(d): os.makedirs(d)\n",
    "\n",
    "# ================= 2. MODEL LOADING =================\n",
    "print(\"‚è≥ Loading Models...\")\n",
    "seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
    "seg_model.eval()\n",
    "\n",
    "if os.path.exists(Config.MODEL_PATH):\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    diffusion_model = UNet2DModel.from_pretrained(Config.MODEL_PATH).to(device)\n",
    "    diffusion_model.eval()\n",
    "    print(\"‚úÖ Models Ready!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Diffusion\")\n",
    "\n",
    "# ================= 3. HELPER FUNCTIONS =================\n",
    "def get_masks(img_numpy_uint8):\n",
    "    try:\n",
    "        h, w = img_numpy_uint8.shape\n",
    "        img_norm = xrv.datasets.normalize(img_numpy_uint8, 255) \n",
    "        img_resized = cv2.resize(img_norm, (512, 512))\n",
    "        img_tensor = torch.from_numpy(img_resized)[None, None, ...].float()\n",
    "        with torch.no_grad(): outputs = seg_model(img_tensor)\n",
    "        pred = outputs[0].numpy()\n",
    "        def resize_m(m): return cv2.resize(m, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        mask_l = (resize_m(pred[4]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_r = (resize_m(pred[5]) > 0.5).astype(np.uint8) * 255\n",
    "        mask_total = cv2.bitwise_or(mask_l, mask_r)\n",
    "        \n",
    "        return mask_total, mask_l, mask_r\n",
    "    except:\n",
    "        z = np.zeros(img_numpy_uint8.shape, dtype=np.uint8)\n",
    "        return z, z, z\n",
    "\n",
    "def enhance_lung_clarity(img_gray, lung_mask):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(img_gray)\n",
    "    return cv2.bitwise_and(enhanced, enhanced, mask=lung_mask)\n",
    "\n",
    "def process_image(img_path):\n",
    "    if not os.path.exists(img_path): return None, None, None, None, None\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: return None, None, None, None, None\n",
    "    \n",
    "    mask_total, mask_l, mask_r = get_masks(img)\n",
    "    \n",
    "    def resize_binary(m):\n",
    "        m_res = cv2.resize(m, (Config.IMG_SIZE, Config.IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "        return (m_res > 127).astype(np.float32)\n",
    "\n",
    "    mask_binary_total = resize_binary(mask_total)\n",
    "    mask_binary_l = resize_binary(mask_l)\n",
    "    mask_binary_r = resize_binary(mask_r)\n",
    "\n",
    "    if np.sum(mask_total) == 0: enhanced = img\n",
    "    else: enhanced = enhance_lung_clarity(img, mask_total)\n",
    "        \n",
    "    resized = cv2.resize(enhanced, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "    img_tensor = torch.tensor(resized).float() / 255.0\n",
    "    \n",
    "    return img_tensor.unsqueeze(0).unsqueeze(0).to(device), img, mask_binary_total, mask_binary_l, mask_binary_r\n",
    "\n",
    "# ================= 3.1 UPDATED METRICS CALCULATION =================\n",
    "def calculate_lung_metrics(diff_map, mask_binary, img_orig_norm):\n",
    "    \"\"\"\n",
    "    diff_map: ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á (Difference Map)\n",
    "    mask_binary: ‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å‡∏õ‡∏≠‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à (0 ‡∏´‡∏£‡∏∑‡∏≠ 1)\n",
    "    img_orig_norm: ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß (0.0 - 1.0)\n",
    "    \"\"\"\n",
    "    # 1. Basic Volume\n",
    "    lung_pixels = np.sum(mask_binary)\n",
    "    if lung_pixels == 0:\n",
    "        return {\n",
    "            \"Affected\": \"No\", \"Opacity_Score\": 0, \"Lung_Volume_px\": 0, \"Volume_Opacity_px\": 0, \n",
    "            \"Pct_Opacity\": 0.0, \"Volume_High_Opacity_px\": 0, \"Pct_High_Opacity\": 0.0,\n",
    "            \"Mean_Intensity\": 0.0, \"Mean_Intensity_Opacity\": 0.0, \"StdDev_Total\": 0.0, \"StdDev_Opacity\": 0.0\n",
    "        }\n",
    "\n",
    "    # 2. Opacity (General) - ‡πÉ‡∏ä‡πâ HEATMAP_THRESHOLD\n",
    "    diff_inside = diff_map * mask_binary\n",
    "    opacity_mask = (diff_inside > Config.HEATMAP_THRESHOLD).astype(np.float32)\n",
    "    opacity_pixels = np.sum(opacity_mask)\n",
    "    pct_opacity = (opacity_pixels / lung_pixels) * 100\n",
    "    \n",
    "    # 3. High Opacity (Severe) - ‡πÉ‡∏ä‡πâ CONTOUR_THRESHOLD\n",
    "    high_opacity_mask = (diff_inside > Config.CONTOUR_THRESHOLD).astype(np.float32)\n",
    "    high_opacity_pixels = np.sum(high_opacity_mask)\n",
    "    pct_high_opacity = (high_opacity_pixels / lung_pixels) * 100\n",
    "\n",
    "    # 4. Opacity Score logic\n",
    "    if pct_opacity <= 1: op_score = 0\n",
    "    elif pct_opacity <= 25: op_score = 1\n",
    "    elif pct_opacity <= 50: op_score = 2\n",
    "    elif pct_opacity <= 75: op_score = 3\n",
    "    else: op_score = 4\n",
    "\n",
    "    # 5. Intensity & StdDev Stats\n",
    "    # ‡πÄ‡∏≠‡∏≤‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Mean/Std ‡∏Ç‡∏≠‡∏á \"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏õ‡∏≠‡∏î‡πÄ‡∏î‡∏¥‡∏°\"\n",
    "    lung_values = img_orig_norm[mask_binary > 0]\n",
    "    mean_intensity = np.mean(lung_values) if len(lung_values) > 0 else 0\n",
    "    std_total = np.std(lung_values) if len(lung_values) > 0 else 0\n",
    "\n",
    "    # ‡πÄ‡∏≠‡∏≤‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≠‡∏¢‡πÇ‡∏£‡∏Ñ (‡∏à‡∏≤‡∏Å Diff Map ‡∏´‡∏£‡∏∑‡∏≠ Original ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡πÅ‡∏ô‡πà‡∏ô‡∏ù‡πâ‡∏≤ ‡∏°‡∏±‡∏Å‡∏î‡∏π‡∏ó‡∏µ‡πà Diff Map ‡∏´‡∏£‡∏∑‡∏≠ Original ‡∏ï‡∏£‡∏á‡∏ù‡πâ‡∏≤)\n",
    "    # *‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Difference Map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∑‡πà‡∏≠‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á‡∏ù‡πâ‡∏≤‡∏ó‡∏µ‡πà AI ‡πÄ‡∏´‡πá‡∏ô*\n",
    "    opacity_values = diff_inside[opacity_mask > 0]\n",
    "    mean_int_op = np.mean(opacity_values) if len(opacity_values) > 0 else 0\n",
    "    std_op = np.std(opacity_values) if len(opacity_values) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Affected\": \"Yes\" if pct_opacity > 1 else \"No\",\n",
    "        \"Opacity_Score\": op_score,\n",
    "        \"Lung_Volume_px\": int(lung_pixels),\n",
    "        \"Volume_Opacity_px\": int(opacity_pixels),\n",
    "        \"Pct_Opacity\": round(pct_opacity, 2),\n",
    "        \"Volume_High_Opacity_px\": int(high_opacity_pixels),\n",
    "        \"Pct_High_Opacity\": round(pct_high_opacity, 2),\n",
    "        \"Mean_Intensity\": round(mean_intensity, 4),           # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏õ‡∏≠‡∏î\n",
    "        \"Mean_Intensity_Opacity\": round(mean_int_op, 4),      # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏≠‡∏¢‡πÇ‡∏£‡∏Ñ\n",
    "        \"StdDev_Total\": round(std_total, 4),                  # StdDev ‡∏õ‡∏≠‡∏î‡∏£‡∏ß‡∏°\n",
    "        \"StdDev_Opacity\": round(std_op, 4)                    # StdDev ‡∏£‡∏≠‡∏¢‡πÇ‡∏£‡∏Ñ\n",
    "    }\n",
    "\n",
    "# ================= 4. CORE FUNCTION =================\n",
    "def analyze_lesion_score(img_path, model, scheduler, start_timestep=180, seed=42):\n",
    "    # 1. Process\n",
    "    input_tensor, original_img_np, mask_total, mask_l, mask_r = process_image(img_path)\n",
    "    if input_tensor is None: return None, None, None, None, None\n",
    "\n",
    "    # Locus Seed\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Diffusion\n",
    "    noise = torch.randn(input_tensor.shape).to(device)\n",
    "    timesteps = torch.tensor([start_timestep], device=device).long()\n",
    "    noisy_image = scheduler.add_noise(input_tensor, noise, timesteps)\n",
    "\n",
    "    current_image = noisy_image\n",
    "    scheduler_timesteps = scheduler.timesteps\n",
    "    start_index = (scheduler_timesteps == start_timestep).nonzero(as_tuple=True)[0].item()\n",
    "    subset_timesteps = scheduler_timesteps[start_index:]\n",
    "\n",
    "    for t in subset_timesteps:\n",
    "        with torch.no_grad():\n",
    "            model_output = model(current_image, t).sample\n",
    "            current_image = scheduler.step(model_output, t, current_image).prev_sample\n",
    "\n",
    "    # Prepare Images\n",
    "    img_recon = current_image.cpu().numpy()[0, 0]\n",
    "    img_orig = input_tensor.cpu().numpy()[0, 0]\n",
    "    img_orig = (img_orig + 1) / 2\n",
    "    img_recon = (img_recon - img_recon.min()) / (img_recon.max() - img_recon.min())\n",
    "    \n",
    "    img_orig_blur = cv2.GaussianBlur(img_orig, (3, 3), 0)\n",
    "    \n",
    "    # Diff Calculation\n",
    "    diff_raw = np.abs(img_orig_blur - img_recon)\n",
    "    diff_clean = diff_raw * mask_total\n",
    "\n",
    "    # Calculate Detailed Metrics\n",
    "    metrics_total = calculate_lung_metrics(diff_raw, mask_total, img_orig)\n",
    "    metrics_left = calculate_lung_metrics(diff_raw, mask_l, img_orig)\n",
    "    metrics_right = calculate_lung_metrics(diff_raw, mask_r, img_orig)\n",
    "    \n",
    "    # Score Calculation (MAE)\n",
    "    lung_pixel_count = metrics_total[\"Lung_Volume_px\"]\n",
    "    if lung_pixel_count == 0: score_mae = 0.0\n",
    "    else: score_mae = (np.sum(diff_clean) / lung_pixel_count) * 100\n",
    "\n",
    "    # ================= VISUALIZATION =================\n",
    "    max_diff = np.max(diff_clean)\n",
    "    norm_diff = (diff_clean / max_diff) if max_diff > 0 else diff_clean\n",
    "    norm_diff = np.where(norm_diff < Config.HEATMAP_THRESHOLD, 0, norm_diff)\n",
    "\n",
    "    heatmap_uint8 = (norm_diff * 255).astype(np.uint8)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "    \n",
    "    orig_uint8 = (img_orig * 255).astype(np.uint8)\n",
    "    orig_bgr = cv2.cvtColor(orig_uint8, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    heatmap_mask = (norm_diff > 0).astype(np.float32)[:, :, None]\n",
    "    overlay = orig_bgr.astype(np.float32) * (1 - heatmap_mask * 0.4) + \\\n",
    "              heatmap_color.astype(np.float32) * (heatmap_mask * 0.4)\n",
    "    overlay = overlay.astype(np.uint8)\n",
    "\n",
    "    # Contours\n",
    "    high_opacity_map = (norm_diff > Config.CONTOUR_THRESHOLD).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(high_opacity_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(overlay, contours, -1, (0, 255, 255), 1) \n",
    "\n",
    "    # --- üõ†Ô∏è FIX: Change Background to Black ---\n",
    "    outside_mask = (mask_total == 0)\n",
    "    overlay[outside_mask] = 0 # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏µ‡∏î‡∏≥‡∏™‡∏ô‡∏¥‡∏ó [0,0,0]\n",
    "\n",
    "    return overlay, score_mae, img_orig, img_recon, (metrics_total, metrics_left, metrics_right)\n",
    "\n",
    "# ================= 5. COMPARISON FUNCTION =================\n",
    "def generate_comparison_plot(res1, res2, pid, cls_name, dates):\n",
    "    overlay1, score1, orig1, recon1, _ = res1\n",
    "    overlay2, score2, orig2, recon2, _ = res2\n",
    "    \n",
    "    diff_score = score2 - score1\n",
    "    if diff_score < 0: status = \"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô (Improved)\"; color = 'green'\n",
    "    elif diff_score > 0: status = \"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà‡∏•‡∏á (Worsened)\"; color = 'red'\n",
    "    else: status = \"‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏á‡∏ï‡∏±‡∏ß (Stable)\"; color = 'blue'\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    axes[0, 0].imshow(orig1, cmap='gray'); axes[0, 0].set_title(f\"Day 1 ({dates[0]})\", fontsize=12)\n",
    "    axes[0, 1].imshow(recon1, cmap='gray'); axes[0, 1].set_title(\"Day 1: AI Reconstructed\", fontsize=12)\n",
    "    axes[0, 2].imshow(cv2.cvtColor(overlay1, cv2.COLOR_BGR2RGB)) \n",
    "    axes[0, 2].set_title(f\"Score: {score1:.2f} (MAE)\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    axes[1, 0].imshow(orig2, cmap='gray'); axes[1, 0].set_title(f\"Day Last ({dates[1]})\", fontsize=12)\n",
    "    axes[1, 1].imshow(recon2, cmap='gray'); axes[1, 1].set_title(\"Day Last: AI Reconstructed\", fontsize=12)\n",
    "    axes[1, 2].imshow(cv2.cvtColor(overlay2, cv2.COLOR_BGR2RGB)) \n",
    "    axes[1, 2].set_title(f\"Score: {score2:.2f} (MAE)\", fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    for ax in axes.flat: ax.axis('off')\n",
    "    plt.suptitle(f\"Patient {pid} Progress: {status} (Diff: {diff_score:+.2f})\", fontsize=16, color=color, y=1)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(Config.DIRS[\"comparisons\"], f\"{cls_name}_{pid}_Compare.jpg\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ================= 6. EXECUTION LOOP =================\n",
    "def get_timeline_files(folder):\n",
    "    files = glob.glob(os.path.join(folder, \"*.jpg\"))\n",
    "    daily = {}\n",
    "    for f in files:\n",
    "        try:\n",
    "            parts = os.path.basename(f).replace(\".jpg\", \"\").split(\"_\")\n",
    "            d = datetime.strptime(parts[-2], \"%Y%m%d\")\n",
    "            s = int(parts[-1])\n",
    "            # ‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà sequence ‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î (‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô)\n",
    "            if d not in daily or s > daily[d][0]: daily[d] = (s, f)\n",
    "        except: continue\n",
    "    return [daily[d][1] for d in sorted(daily)], sorted(daily)\n",
    "\n",
    "all_csv_data = [] \n",
    "print(\"üöÄ Starting Analysis with Detailed CSV & Black Background...\")\n",
    "\n",
    "for cls in [\"novap\", \"vap\"]:\n",
    "    c_path = os.path.join(Config.ROOT_DIR, cls)\n",
    "    if not os.path.exists(c_path): continue\n",
    "    \n",
    "    for pid in os.listdir(c_path):\n",
    "        p_path = os.path.join(c_path, pid)\n",
    "        files, dates = get_timeline_files(p_path)\n",
    "        if not files: continue\n",
    "        \n",
    "        print(f\"Processing: {pid} ({len(files)} scans)\")\n",
    "        patient_results = []\n",
    "        patient_dates = []\n",
    "        mae_scores = []\n",
    "        \n",
    "        # Dictionary to track duplicate dates for display\n",
    "        date_counts = {}\n",
    "\n",
    "        for i, f in enumerate(files):\n",
    "            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Date String ‡∏ñ‡πâ‡∏≤‡∏ã‡πâ‡∏≥ (‡πÅ‡∏°‡πâ get_timeline_files ‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏ó‡∏≥‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ß‡πâ)\n",
    "            d_obj = dates[i]\n",
    "            base_date_s = d_obj.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            if base_date_s not in date_counts:\n",
    "                date_counts[base_date_s] = 1\n",
    "                final_date_str = base_date_s\n",
    "            else:\n",
    "                date_counts[base_date_s] += 1\n",
    "                final_date_str = f\"{base_date_s}({date_counts[base_date_s]})\"\n",
    "\n",
    "            res = analyze_lesion_score(f, diffusion_model, noise_scheduler, start_timestep=Config.START_TIMESTEP, seed=Config.SEED)\n",
    "            \n",
    "            overlay, score_mae, orig, recon, detailed_metrics = res\n",
    "            \n",
    "            if overlay is not None:\n",
    "                patient_results.append(res)\n",
    "                patient_dates.append(final_date_str)\n",
    "                mae_scores.append(score_mae)\n",
    "                \n",
    "                m_total, m_left, m_right = detailed_metrics\n",
    "                \n",
    "                # Save Image\n",
    "                fname = f\"{pid}_{final_date_str.replace('/', '-')}.jpg\"\n",
    "                save_dir = os.path.join(Config.DIRS[\"heatmaps\"], cls, pid)\n",
    "                if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "                cv2.imwrite(os.path.join(save_dir, fname), overlay)\n",
    "                \n",
    "                # --- PREPARE CSV ROW ---\n",
    "                row = {\n",
    "                    \"Patient_ID\": pid,\n",
    "                    \"Date\": final_date_str,\n",
    "                    \"Group\": cls,\n",
    "                    \n",
    "                    # 2. Overall Scores\n",
    "                    \"Total Opacity Score (Sum L+R)\": m_left[\"Opacity_Score\"] + m_right[\"Opacity_Score\"],\n",
    "                    \"Total % Opacity\": m_total[\"Pct_Opacity\"],\n",
    "                }\n",
    "\n",
    "                # 3. Add Detailed Metrics (Flattening dicts)\n",
    "                # Helper function to add prefix\n",
    "                def add_metrics(prefix, m_dict):\n",
    "                    for k, v in m_dict.items():\n",
    "                        row[f\"{prefix}_{k}\"] = v\n",
    "                \n",
    "                add_metrics(\"Both\", m_total)\n",
    "                add_metrics(\"Left\", m_left)\n",
    "                add_metrics(\"Right\", m_right)\n",
    "                \n",
    "                all_csv_data.append(row)\n",
    "\n",
    "        # Timeline generation (Same as before)\n",
    "        if len(patient_results) >= 2:\n",
    "            generate_comparison_plot(patient_results[0], patient_results[-1], pid, cls, [patient_dates[0], patient_dates[-1]])\n",
    "        \n",
    "        if len(patient_results) > 0:\n",
    "            plt.figure(figsize=(4 * len(patient_results), 8))\n",
    "            for i in range(len(patient_results)):\n",
    "                ax = plt.subplot(2, len(patient_results), i+1)\n",
    "                ax.imshow(cv2.cvtColor(patient_results[i][0], cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                current_score = mae_scores[i]\n",
    "                if i == 0: color = 'blue'\n",
    "                else:\n",
    "                    prev_score = mae_scores[i-1]\n",
    "                    if current_score < prev_score: color = 'green'\n",
    "                    elif current_score > prev_score: color = 'red'\n",
    "                    else: color = 'blue'\n",
    "                ax.set_title(f\"{patient_dates[i]}\\nScore: {current_score:.2f}\", color=color, fontweight='bold', fontsize=14)\n",
    "                ax.axis('off')\n",
    "            \n",
    "            ax2 = plt.subplot(2, 1, 2)\n",
    "            ax2.plot(patient_dates, mae_scores, 'b-o', linewidth=2)\n",
    "            ax2.set_ylabel(\"Opacity Score (MAE)\")\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            plt.suptitle(f\"Timeline: {pid} ({cls})\", fontsize=16)\n",
    "            plt.savefig(os.path.join(Config.DIRS[\"timelines\"], f\"{cls}_{pid}_Timeline.jpg\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "if all_csv_data:\n",
    "    df = pd.DataFrame(all_csv_data)\n",
    "    \n",
    "    # ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö Column ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡∏≠ (Optional sorting)\n",
    "    first_cols = [\"Patient_ID\", \"Date\", \"Group\", \"Total Opacity Score (Sum L+R)\", \"Total % Opacity\"]\n",
    "    other_cols = [c for c in df.columns if c not in first_cols]\n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á other_cols ‡πÉ‡∏´‡πâ Both ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô Left ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô Right\n",
    "    other_cols.sort(key=lambda x: (0 if \"Both\" in x else 1 if \"Left\" in x else 2, x))\n",
    "    \n",
    "    df = df[first_cols + other_cols]\n",
    "\n",
    "    csv_path = os.path.join(Config.DIRS[\"csv\"], \"Clinical_Data_Detailed.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n‚úÖ Final Clinical Report Saved: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a85663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
